name: "tokenizer"
backend: "python"
max_batch_size: 4096

input [
  {
    name: "INPUT_TEXT"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
input [
  {
    name: "INPUT_LANGUAGE_ID"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
input [
  {
    name: "OUTPUT_LANGUAGE_ID"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]

output [
  {
    name: "INPUT_TEXT_TOKENIZED"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]

dynamic_batching {}

instance_group [
  {
    count: 8
    kind: KIND_CPU
  }
]
